# What's wrong with regular autoencoders?
Autoencoders do a fine job of compressing data, and building it back up. Often you will hear the term dimensionalty reduction, which occurs in the middle layer, where the network has found an optimum transformation to reduce the dimensions, such that most the of the original information is preserved. However imagine you want to use the autoencoder to generate data. You might think that you can walk around the latent space of the autoencoder and get new and interesing results, if you do try this you will be a bit dissapointed. Often a lot of the space you traverse will be dead space, meaning that the decoder of the autoencoder will not produce anything meaningful. There is a clear reason to why this is: there is no incentive to produce a compact latent space. This is what VAE's try to solve, and they indeed do a good job. 

