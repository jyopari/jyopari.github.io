<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Towards Self-Editing Models : Part 1</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
	margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(0, 0, 0, 0.06); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="180e3cb2-6055-809e-a80f-fff10273ff23" class="page sans"><header><h1 class="page-title">Towards Self-Editing Models : Part 1</h1><p class="page-description"></p></header><div class="page-body"><p id="180e3cb2-6055-8081-b316-c077730a1605" class=""><strong>Motivation</strong></p><p id="180e3cb2-6055-8077-933b-cbffc4d24ae6" class="">A recent trend in machine learning research has focused on <strong>test-time training</strong> and, more broadly, on methods that allow models to adapt to new examples. One particularly intriguing direction in this area is enabling models to <strong>self-update</strong>, removing the need for external optimization processes. Although appealing in theory, accomplishing this is non-trivial, and in this article I propose thinking of more expressive self-updating mechanisms. </p><p id="180e3cb2-6055-801a-91df-f9713299bc29" class=""><strong>Related Work on Self-Updating Mechanisms</strong></p><p id="180e3cb2-6055-8055-b33d-f37550605380" class="">Several lines of work provide insights into how models might update themselves. For instance, my colleague Ekin has a very interesting paper, <a href="https://openreview.net/pdf?id=0g0X4H8yN4I">&quot;What Learning Algorithm Is In-Context Learning? Investigations with Linear Models&quot;</a>. The authors demonstrate that:</p><blockquote id="180e3cb2-6055-80f7-ac27-d5e9757febae" class="">&quot;Transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression.&quot;</blockquote><p id="180e3cb2-6055-80dd-8864-d7789b1d44cb" class="">Another paper I like is <a href="https://arxiv.org/abs/2307.01189">&quot;Trainable Transformer in Transformer&quot;</a> by Sanjeev Arora’s group, which shows how the forward, backward, and update steps of a smaller transformer can be <em>simulated</em> within a single forward pass of a larger transformer.</p><p id="180e3cb2-6055-80cd-b3c2-d59711bdf992" class="">These approaches, while highly creative, prompt an important question: <strong>What if we want the inner model to grow—i.e., to capture an increasingly complex function class over time?</strong></p><hr id="180e3cb2-6055-8082-a368-f3853e34bd53"/><p id="180e3cb2-6055-803e-87da-cf1a56501e0b" class=""><strong>Conceptual Framework: Growing Models</strong></p><p id="180e3cb2-6055-809b-a207-c20d978a1473" class="">Consider the diagram below:</p><figure id="180e3cb2-6055-8008-a116-e46fad6c95ea" class="image"><a href="Towards%20Self-Editing%20Models%20Part%201%20180e3cb26055809ea80ffff10273ff23/Screenshot_2025-01-16_at_3.36.43_PM.png"><img style="width:480px" src="Towards%20Self-Editing%20Models%20Part%201%20180e3cb26055809ea80ffff10273ff23/Screenshot_2025-01-16_at_3.36.43_PM.png"/></a><figcaption>On the <strong>left</strong>, the larger model, denoted by φ, is responsible for optimizing the parameters of the inner model, θ. If we want θ to become more complex over time, φ must also grow to support and guide that increased complexity. This nesting of inner and outer models introduces significant challenges: the update mechanism for θ is bound to φ’s capacity to represent and manipulate growing parameter sets.</figcaption></figure><p id="180e3cb2-6055-8008-b76d-f4563a0abc30" class=""><strong>Towards a Single Self-Updating Model</strong></p><p id="180e3cb2-6055-8065-852e-d6f17a31a23b" class="">Rather than maintaining a nested structure (<style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">ϕ</span></span></span></span></span><span>﻿</span></span>  updating <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span></span><span>﻿</span></span>), we can envision a single model that <strong>self-updates</strong>—and even grows—entirely on its own. If we rely solely on gradient descent as the desired update, the model can only update a small sub-component of itself at each step as shown by the aforementioned papers. Yet gradient descent is not our only option. In principle, a model could learn other and efficient mechanism for self-updating, potentially bypassing the limitations of an external optimizer.</p><p id="180e3cb2-6055-80ef-85a8-f928324725f9" class="">With that in mind, the following sections explore how transformers, chain-of-thought (CoT) reasoning, and related paradigms might inspire the design of architectures that adapt and expand.</p><hr id="180e3cb2-6055-8041-afd5-d8b0bb3a8e94"/><p id="180e3cb2-6055-80da-b148-c49219d3ac57" class=""><strong>The Case with Transformers and Chain-of-Thought (CoT)</strong></p><figure id="180e3cb2-6055-804d-a60f-f07980ea0702" class="image"><a href="Towards%20Self-Editing%20Models%20Part%201%20180e3cb26055809ea80ffff10273ff23/Screenshot_2025-01-19_at_6.30.03_PM.png"><img style="width:192px" src="Towards%20Self-Editing%20Models%20Part%201%20180e3cb26055809ea80ffff10273ff23/Screenshot_2025-01-19_at_6.30.03_PM.png"/></a></figure><p id="180e3cb2-6055-8001-9069-c68526b48764" class="">If we think about modeling any function with a Transformer, one line of thought is can they behave like interpreter and compute any inputted program? To this end, a thought-provoking paper, “<a href="https://arxiv.org/abs/2310.07923">The Expressive Power of Transformers with Chain of Thought</a>” by Will Merrill, demonstrates that with slight modifications to the standard transformer decoder, one can simulate both finite automata and Turing machines. In this analogy, the CoT reasoning serves as the Turing machine’s tape, while the MLP block encodes the transition function—the “program” being executed.</p><p id="180e3cb2-6055-8055-ab4a-d748621cc17d" class="">However, a critical limitation arises from the static nature of the MLP block. Because its parameters remain fixed during inference, the program encoded by the transformer cannot modify itself dynamically. The model can simulate pre-defined computations but cannot <em>self-edit</em> its own transitions or logic. This leads us to the question:</p><blockquote id="180e3cb2-6055-808d-8ad9-fd7789506fed" class="">What can transformers model over time if they cannot change their own weights - but only the context. </blockquote><hr id="180e3cb2-6055-80aa-a764-f7810b1dc334"/><p id="180e3cb2-6055-8058-84dd-d4d4e89e83e4" class=""><strong>Transformer Dynamics and Computational Scaling</strong></p><p id="180e3cb2-6055-80db-8b06-e06682551650" class="">In a standard transformer decoder generating text auto-regressively, each predicted token must attend to all previous tokens. This implies that the amount of computation grows with the sequence length. Specifically, the self-attention mechanism <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi><mo stretchy="false">(</mo><mi>q</mi><msup><mi>K</mi><mi mathvariant="normal">⊤</mi></msup><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\sigma(qK^\top)V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0991em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span><span>﻿</span></span> includes an increasingly large context as the sequence unfolds. Consequently, the model can represent increasingly complex functions simply by attending to more tokens - hence the higher FLOPs that are paid.</p><p id="180e3cb2-6055-800e-b6ad-e1ec5b0a84d7" class="">Even in simplified cases—such as “hard” attention where <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span><span>﻿</span></span> self-attention can theoretically model arbitrary mappings by acting like a hash table. With the addition of the MLP block, the model can transform these mappings into richer representations. Stacking multiple layers adds further expressive power. Below is an illustrative example showing how <strong>linear attention</strong> can be formulated in a recurrent manner, to provide perspective on how the function changes. </p><figure id="180e3cb2-6055-80c5-9ce9-fdbf1f58cf74" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mrow><mtext mathvariant="bold">Linear</mtext><mtext> </mtext><mtext mathvariant="bold">(no-softmax)</mtext><mtext> </mtext><mtext mathvariant="bold">attention</mtext><mtext> </mtext><mtext mathvariant="bold">recurrence:</mtext></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><msub><mi>U</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mtext>  </mtext><mo>=</mo><mtext>  </mtext><msub><mi>U</mi><mi>t</mi></msub><mtext>  </mtext><mo>+</mo><mtext>  </mtext><msub><mi>k</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mtext> </mtext><msubsup><mi>v</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi mathvariant="normal">⊤</mi></msubsup><mo separator="true">,</mo><mspace width="1em"/><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mtext>  </mtext><mo>=</mo><mtext>  </mtext><msub><mi>w</mi><mi>t</mi></msub><mtext>  </mtext><mo>+</mo><mtext>  </mtext><msub><mi>k</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><msub><mi>O</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mtext>  </mtext><mo>=</mo><mtext>  </mtext><mfrac><mrow><mtext> </mtext><msubsup><mi>q</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi mathvariant="normal">⊤</mi></msubsup><mtext> </mtext><msub><mi>U</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mtext> </mtext></mrow><mrow><mtext> </mtext><msubsup><mi>q</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi mathvariant="normal">⊤</mi></msubsup><mtext> </mtext><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mtext> </mtext></mrow></mfrac><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
&amp;\textbf{Linear (no-softmax) attention recurrence:}\\[6pt]
&amp;U_{t+1} \;=\; U_t \;+\; k_{t+1}\,v_{t+1}^\top,
\quad
w_{t+1} \;=\; w_t \;+\; k_{t+1},\\[4pt]
&amp;O_{t+1}
\;=\;
\frac{\,q_{t+1}^\top\,U_{t+1}\,}{\,q_{t+1}^\top\,w_{t+1}\,}.
\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:6.9502em;vertical-align:-3.2251em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.7251em;"><span style="top:-6.4306em;"><span class="pstrut" style="height:3.5455em;"></span><span class="mord"></span></span><span style="top:-4.2715em;"><span class="pstrut" style="height:3.5455em;"></span><span class="mord"></span></span><span style="top:-1.666em;"><span class="pstrut" style="height:3.5455em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.2251em;"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.7251em;"><span style="top:-6.4306em;"><span class="pstrut" style="height:3.5455em;"></span><span class="mord"><span class="mord"></span><span class="mord text"><span class="mord textbf">Linear (no-softmax) attention recurrence:</span></span></span></span><span style="top:-4.2715em;"><span class="pstrut" style="height:3.5455em;"></span><span class="mord"><span class="mord"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3053em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mpunct">,</span></span></span><span style="top:-1.666em;"><span class="pstrut" style="height:3.5455em;"></span><span class="mord"><span class="mord"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5455em;"><span style="top:-2.2791em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8309em;"><span style="top:-2.4337em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.0448em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3246em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.6964em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-2.4519em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3064em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0455em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord">.</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.2251em;"><span></span></span></span></span></span></span></span></span></span></span></span></div></figure><p id="180e3cb2-6055-80c5-afb6-de7ddd2c82d2" class="">Although it differs from standard softmax-based self-attention, it highlights how the function changes when a new token is added. And the direction I want to think about is how can we make more expressive function updates. </p><p id="180e3cb2-6055-801b-a602-d77913e802be" class="">A  notable constraint of standard transformers lies in their inability to revise their context history. This is analogous to writing code sequentially <strong>without ever returning to edit earlier lines</strong>. In contrast, a human programmer routinely refines and updates previous code to correct errors or optimize performance. Transformers, however, process the context in a strictly forward manner, potentially limiting their capacity.</p><hr id="180e3cb2-6055-8092-a153-dcdf6c3705b2"/><p id="180e3cb2-6055-80c0-9b81-f8bad13fd061" class=""><strong>The</strong><strong> End Goal</strong></p><p id="180e3cb2-6055-8056-a277-fd8282c638cd" class="">Ultimately, we want a model that runs and <strong>continually edits itself</strong>—much like a Python script that automatically modifies its own source file at the end of each execution. In deep learning terms, the challenge is to design and train models that can modify both their parameters and their computational structure over time, without relying solely on an external optimizer.</p><p id="180e3cb2-6055-8091-b329-e3725a033202" class="">The key question thus becomes:</p><blockquote id="180e3cb2-6055-80f0-89bb-e7005ce1df9e" class="">How do we equip transformers—or any deep learning architecture—with the capacity to function like a self-editing program?</blockquote><p id="180e3cb2-6055-80a2-915a-c1c6212360e2" class="">Addressing this question requires breaking away from purely static architectures and exploring mechanisms that allow models to rewrite both their intermediate states and the “program” they encode. Below I listed some directions I think are worth considering. </p><hr id="180e3cb2-6055-80e5-b877-d688cfb212ed"/><p id="180e3cb2-6055-804b-b687-f7f4c5402e74" class=""><strong>Directions Forward</strong></p><p id="180e3cb2-6055-80dc-9297-fffde5eb1225" class=""><strong>Modular Networks for Self-Editing</strong>: Modular networks decompose a model into subroutines—akin to functions in a programming language—each with a stable, well-defined interface. A controller oversees how these modules are created, combined, or replaced, enabling the system to incrementally build more complex functions. The “edit” parameterization becomes a blueprint for constructing new modules or inserting function calls, allowing the model to expand its functionality over time while preserving robustness and clarity at the interfaces.</p><hr id="180e3cb2-6055-80ea-9740-caf1c9a4a37c"/><p id="180e3cb2-6055-80f1-b831-d2b62bb966e8" class=""><strong>Token-Driven Updates</strong></p><p id="180e3cb2-6055-80ba-9875-cd2eb84ca71a" class="">A powerful approach is to treat tokens themselves as <strong>executable instructions</strong> that directly modify the model’s function. Rather than mere following the fixed computation of self attention, these “edit tokens” are interpreted by a specialized mechanism or “interpreter,” which translates them into structural or parameter updates. By embedding edits into the model’s own generation process, the system gains <strong>in-context self-modification</strong> capabilities: it can create new functions, alter module connections, or adjust parameters on the fly. This mechanism mirrors how a programmer edits code—except that the instructions here are produced and consumed within the same architecture. Key challenges include ensuring valid updates, preventing runaway changes, and balancing the expressive power of edit tokens with the stability needed for reliable execution.</p><hr id="180e3cb2-6055-8028-afed-c2c926942c8f"/><p id="180e3cb2-6055-80dd-a024-c5259fbb4cc9" class=""><strong>Differentiable Parameterized Edits</strong></p><p id="180e3cb2-6055-8091-9ff2-c4f163b3cdb7" class="">Up to now, we’ve mainly considered <strong>discrete</strong> structural edits—like adding or removing layers. But could we instead <strong>continuously</strong> modify a network’s architecture, beyond just adjusting parameters? For instance, can we devise a gradient-based process that gradually morphs a model’s structure—expanding or contracting its capacity in a smooth, differentiable manner? This might merge the interpretability of explicit structural changes with the finer control of continuous optimization, opening new pathways for self-editing architectures.</p><hr id="180e3cb2-6055-80ba-a738-fb8828fed3e2"/><p id="180e3cb2-6055-80ca-a895-d84d82636eb9" class=""><strong>Summary</strong></p><p id="180e3cb2-6055-80b2-be43-c0f640cc6ac9" class="">Parameterizing a self-editable model opens new frontiers in machine learning. Meta-learning training frameworks can equip these models to adapt their own update rules, but achieving scalability and reliability demands <strong>efficient</strong> approaches to meta-optimization and thinking of alternatives. Striking the right balance between complexity, interpretability, and stability remains an open challenge.</p><p id="180e3cb2-6055-8090-a344-efd80025b2df" class=""><strong>Key Takeaway</strong>: I think the key punch lines I am thinking about is: </p><ul id="180e3cb2-6055-8055-bb65-e6d8d27d5a50" class="bulleted-list"><li style="list-style-type:disc">A mechanism / parameterization  for specifying edits (the “language” of self-modification).<ul id="180e3cb2-6055-80ec-9bb9-fafd2f0b5df9" class="bulleted-list"><li style="list-style-type:circle">Mathematically, how can we understand the class of functions that can grow over time from a seed function</li></ul></li></ul><hr id="180e3cb2-6055-8051-860d-dd5f33b823b9"/><p id="180e3cb2-6055-8040-9965-e4d763193b9a" class=""><strong>References:</strong></p><ol type="1" id="180e3cb2-6055-800f-8b90-fadcf87613b3" class="numbered-list" start="1"><li>Ekin Akyürek et al., <em><a href="https://openreview.net/pdf?id=0g0X4H8yN4I">&quot;What Learning Algorithm Is In-Context Learning? Investigations with Linear Models&quot;</a></em>, 2022.</li></ol><ol type="1" id="180e3cb2-6055-803f-8160-d712494196cb" class="numbered-list" start="2"><li>Sanjeev Arora et al., <em><a href="https://arxiv.org/abs/2307.01189">&quot;Trainable Transformer in Transformer&quot;</a></em>, 2023.</li></ol><ol type="1" id="180e3cb2-6055-800d-94c7-dea1dea1107e" class="numbered-list" start="3"><li>Will Merrill, <em><a href="https://arxiv.org/abs/2310.07923">&quot;The Expressive Power of Transformers with Chain of Thought&quot;</a></em>, 2023.</li></ol><ol type="1" id="180e3cb2-6055-8067-9622-cd44e6bec938" class="numbered-list" start="4"><li>Katharopoulos et al., <em><a href="https://arxiv.org/pdf/2006.16236.pdf">&quot;Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention&quot;</a></em>, 2020.</li></ol><hr id="180e3cb2-6055-80e0-a577-e59216655953"/><p id="180e3cb2-6055-802e-b38b-dc2d3b291ec8" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>